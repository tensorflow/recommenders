{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vmz2OEZnD5Sl"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "cTmyeW7mD9fd"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vmR0Ojuf6eHq"
      },
      "source": [
        "# Using side information in retrieval\n",
        "\n",
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/movielens_side_information\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/movielens_side_information.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/movielens_side_information.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/movielens_side_information.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s1_278BWHoOe"
      },
      "source": [
        "In the [basic retrieval tutorial](https://github.com/tensorflow/recommenders/blob/main/docs/examples/basic_retrieval.ipynb) we built a simple retrieval model for the Movielens dataset, using user and movie ids as only features.\n",
        "\n",
        "However, it's often useful to use a richer set of features, for both queries and candidates. For example:\n",
        "\n",
        "1. In a large candidate catalogue, there may not be enough data per item to accurately estimate an embedding vector for every item. Using items features (categories, descriptions, images) will help build a model that's more accurate and generalizes better to unseen items.\n",
        "2. Some applications (like fashion) have candidate sets that change frequently, and any given item is available only for a short time. This leaves little time for learning of accurate item representations.\n",
        "3. Users' preferences may change depending on the context they are in. For example, a single user may prefer to consume short-form content when on their phone, and reserve long-form content for their TV. Being able to use the context of the interaction in the model will help capture these nuances.\n",
        "4. Items' relevance may change over time. Including time as an explicit feature will help the model capture popularity trends, preventing items that were once popular but not relevant any more from dominating future recommendations.\n",
        "\n",
        "In this example, we're going to make use of query context to improve our initial model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CBA9KfOdFa2i"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "Let's start with the necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WD2sAmES6dQB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as  tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kxv8x85QIHY1"
      },
      "source": [
        "We also need to repeat the training/test splits and vocabulary building."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uHBSvbkHITdu"
      },
      "outputs": [],
      "source": [
        "ratings = tfds.load(\"movie_lens/100k-ratings\", split=\"train\")\n",
        "movies = tfds.load(\"movie_lens/100k-movies\", split=\"train\")\n",
        "\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_id\": x[\"movie_id\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"timestamp\": x[\"timestamp\"],\n",
        "})\n",
        "movies = movies.map(lambda x: {\n",
        "    \"movie_id\": x[\"movie_id\"],\n",
        "})\n",
        "\n",
        "# Randomly shuffle data and split between train and test.\n",
        "# We fix the random seed to obtain deterministic results.\n",
        "tf.random.set_seed(24)\n",
        "shuffled = ratings.shuffle(100_000, seed=24, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "movie_ids = movies.batch(1_000_000).map(lambda x: x[\"movie_id\"])\n",
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_movie_ids = np.unique(np.concatenate(list(movie_ids)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "14rJMDmfxOT8"
      },
      "source": [
        "## Using query context information\n",
        "\n",
        "The first piece of side information we can make use of is the time when a rating is given. This will help us capture two things:\n",
        "\n",
        "1. Popularity dynamics: some movies are watched a lot when they are first released, but do not become classics. This makes them a good recommendation soon after release, but a bad recommendation later on.\n",
        "2. User tastes changing over time. Making sure our model has the capacity to express this via interactions of user embeddings and time will help us capture this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u117xGdgJS50"
      },
      "source": [
        "### Representing time\n",
        "\n",
        "Our ratings dataset has raw timestamp features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Xu0WwUV0JaRP"
      },
      "outputs": [],
      "source": [
        "for row in ratings.take(2).as_numpy_iterator():\n",
        "  print(f\"Timestamp: {row['timestamp']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zhyRhTPQJtUw"
      },
      "source": [
        "We can't use this directly: we need normalized features in order to make our learning algorithm stable.\n",
        "\n",
        "There are many ways of doing this:\n",
        "1. We could treat time as a continuous feature, and scale it to lie between 0 and 1. This coule be done via quantile scaling (like in sklearn's [QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html)) or through any variety of other methods that map an arbitrary real variable to a fixed interval.\n",
        "2. We could treat time as a discrete variable: a series of discrete time periods.\n",
        "\n",
        "The advantage of treating time as a continuous variable is that its effects become easy to estimate: there are only a few parameters added to the model. However, given enough data, treating time as discrete gives us a more flexible model, where each period as different effects, rather than following a smooth curve over time.\n",
        "\n",
        "In this example, we're going to adopt the discrete approach. We'll start by dividing time into 1000 equal buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lkoxVjgVxg8P"
      },
      "outputs": [],
      "source": [
        "max_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
        "    tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
        "min_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
        "    np.int64(1e9), tf.minimum).numpy().min()\n",
        "\n",
        "timestamp_buckets = np.linspace(\n",
        "    min_timestamp, max_timestamp, num=1000)\n",
        "\n",
        "print(f\"Buckets: {timestamp_buckets[:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JrXeuhFOPORV"
      },
      "source": [
        "### Query model\n",
        "\n",
        "We first define the dimensionality of the query and candidate representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xC-1HkpiPTzZ"
      },
      "outputs": [],
      "source": [
        "embedding_dimension = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaDfwwsJQDOG"
      },
      "source": [
        "We now define the query model. We can map raw features into embeddings via defining appropriate feature columns for our query model.\n",
        "\n",
        "We then use these features in the model. While we could use complex models here, let's start with something simple: we simply concatenate the time and user embedding, and project them onto the item embedding dimension (remember, the output dimensionality between the user model and the candidate model has to be the same)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jKpA9-ytQ0A1"
      },
      "outputs": [],
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, embedding_dimension, timestamp_buckets):\n",
        "    super(UserModel, self).__init__()\n",
        "    # An embedding column for user ids.\n",
        "    user_id_feature = tf.feature_column.embedding_column(\n",
        "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "            \"user_id\", unique_user_ids,\n",
        "        ),\n",
        "        embedding_dimension,\n",
        "    )\n",
        "\n",
        "    # An embedding column for the bucketized timestamps: there will be a separate\n",
        "    # embedding for each of the timestamp buckets.\n",
        "    time_feature = tf.feature_column.embedding_column(\n",
        "        tf.feature_column.bucketized_column(\n",
        "            tf.feature_column.numeric_column(\"timestamp\"),\n",
        "            timestamp_buckets.tolist(),\n",
        "        ),\n",
        "        embedding_dimension,\n",
        "    )\n",
        "    self.embedding_layer = tf.keras.layers.DenseFeatures(\n",
        "        [user_id_feature, time_feature],\n",
        "        name=\"query_embedding\",\n",
        "    )\n",
        "    self.dense_layer = tf.keras.layers.Dense(embedding_dimension)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    input_embedding = self.embedding_layer(inputs)\n",
        "    return self.dense_layer(input_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GtD25VQfRaTO"
      },
      "source": [
        "### Candidate model\n",
        "\n",
        "Let's keep the candidate model as in the [basic retrieval tutorial](https://github.com/tensorflow/recommenders/blob/main/docs/examples/basic_retrieval.ipynb) - using id information only - and see what effect the inclusion of context information has on the model's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pyCLTbDZRjNo"
      },
      "outputs": [],
      "source": [
        "class MovieModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, embedding_dimension):\n",
        "    super(MovieModel, self).__init__()\n",
        "    movie_features = [tf.feature_column.embedding_column(\n",
        "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "            \"movie_id\", unique_movie_ids,\n",
        "        ),\n",
        "        embedding_dimension,\n",
        "    )]\n",
        "    self.embedding_layer = tf.keras.layers.DenseFeatures(movie_features, name=\"movie_embedding\")\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    return self.embedding_layer(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exbGTpplaq2e"
      },
      "source": [
        "### Putting it all together\n",
        "\n",
        "We now put it all together in a model class we'll use for fitting and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-O1DKkxB0E94"
      },
      "outputs": [],
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, embedding_dimension, timestamp_buckets):\n",
        "    super().__init__()\n",
        "    self.user_model: tf.keras.Model = UserModel(embedding_dimension, timestamp_buckets)\n",
        "    self.movie_model: tf.keras.Model = MovieModel(embedding_dimension)\n",
        "    self.task = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=movies.batch(128).map(self.movie_model)\n",
        "        )\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    user_embeddings = self.user_model({\"user_id\": features[\"user_id\"],\n",
        "                                       \"timestamp\": features[\"timestamp\"]})\n",
        "    positive_movie_embeddings = self.movie_model(\n",
        "        {\"movie_id\": features[\"movie_id\"]}\n",
        "    )\n",
        "\n",
        "    return self.task(user_embeddings, positive_movie_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aW63YaqP2wCf"
      },
      "outputs": [],
      "source": [
        "model = MovielensModel(embedding_dimension, timestamp_buckets)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nma0vc2XdN5g"
      },
      "source": [
        "Then shuffle, batch, and cache the training and evaluation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "53QJwY1gUnfv"
      },
      "outputs": [],
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u8mHTxKAdTJO"
      },
      "source": [
        "Then train the  model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZxPntlT8EFOZ"
      },
      "outputs": [],
      "source": [
        "model.fit(cached_train, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uusQ4WcCgSCo"
      },
      "source": [
        "### Results\n",
        "\n",
        "What do the results look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "m6lZCalq1OwO"
      },
      "outputs": [],
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cPBlwExfgU_A"
      },
      "source": [
        "Our accuracies for all values of k are quite a bit better than the model from the basic retrieval tutorial. Clearly, accounting for time is useful.\n",
        "\n",
        "When using timestamp information, the performance of the model varies with the number of buckets we use when bucketizing the timestamps. Hence it is important to choose the right number of buckets.\n",
        "\n",
        "In this tutorial we used 1000 buckets and significantly improved the performance of the model. However, in an experiment with 100 buckets, the model using timestamp information did not perform significantly better than the model from the basic retrieval tutorial."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "movielens_side_information.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
