{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Unified Embedding Tables (UET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/uet\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/uet.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/uet.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/uet.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "metadata": {
        "id": "psHmK78TIkj1"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial demonstrates how to use Unified Embedding to improve performance in recommendation systems.\n",
        "\n",
        "# Background\n",
        "\n",
        "Embeddings are the first and largest component of most recommender models. In a typical recommendation problem, most of the inputs are categorical features that must be embedded as a high-dimensional vector before getting fed into a neural network. The embedding layer is a crucial information bottleneck in the model - if an important relationship isn't represented in the embeddings, the downstream model cannot learn from that signal.\n",
        "\n",
        "**Example:** To make our discussion more concrete, let's imagine that we are building a recommender system for a clothing store. Our goal is to predict the probability that a user will be interested in a particular product, so that we can rank the search results by relevance. Our input features might be the customer's city, the product ID, and the customer's search terms. The supervision signal might be clicks, purchases, or some other measure of user interest. The modeling setup might look something like the following.\n",
        "\n",
        "\u003cdiv\u003e\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://github.com/tensorflow/recommenders/blob/main/assets/embedding_baseline.png?raw=true\" width=\"400\"/\u003e\n",
        "\u003c/center\u003e\n",
        "\u003c/div\u003e\n"
      ]
    },
    {
      "metadata": {
        "id": "4AkrxIvtJNep"
      },
      "cell_type": "markdown",
      "source": [
        "## Why are embeddings challenging to learn?\n",
        "\n",
        "1. **Large vocabulary:** Ideally, we would learn a separate embedding for every feature value, but this is often not possible in production settings. Our clothing store might have an inventory with 100M products - too many to fit a full table into memory.\n",
        "\n",
        "2. **Dynamic vocabulary:** In some applications, new feature values come and go too quickly to keep an up-to-date list of all the values. For example, our clothing store might introduces many new products during the holiday shopping season. To avoid out-of-vocab performance losses and frequent re-trains, we need an embedding algorithm that can handle new values.\n",
        "\n",
        "3. **Hash collisions:** The industry-standard solution to the previous two challenges is to assign feature values to embeddings using a hash function. However, this introduces even more problems stemming from hash collisions. If two values share the same embedding representation, these values become indistinguishable to the model.\n",
        "\n",
        "To see how things could go wrong, suppose that customers in Anchorage, Alaska and Miami, Florida both issue queries including the words \"warm\" and \"weather.\" The customer in Alaska probably wants a warm coat to protect against winter weather, but the Floridian might want clothing appropriate for warm weather in the sub-tropics. However, if Anchorage and Miami collide under the hash function, our model won't be able to distinguish between these search intents. We might accidentally recommend heavy winter coats to customers in Miami."
      ]
    },
    {
      "metadata": {
        "id": "4XsDsWcUD6QY"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What is Unified Embedding?\n",
        "\n",
        "Unified embedding is an approach that uses a single indexing range to hash all of the categorical features in the model. Conceptually, you can think about this as using one massive embedding table for every feature.\n",
        "\n",
        "In practice, we load-balance the embeddings across a few sub-tables because this helps alleviate hot-spot issues and allows us to more easily shard the parameters across accelerators. We also do multiple lookups, so that different features can have embeddings of different dimensions. The unified version of our clothing model might look like the following picture.\n",
        "\n",
        "\u003cdiv\u003e\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src=\"https://github.com/tensorflow/recommenders/blob/main/assets/embedding_unified.png?raw=true\" width=\"400\"/\u003e\n",
        "\u003c/center\u003e\n",
        "\u003c/div\u003e"
      ]
    },
    {
      "metadata": {
        "id": "JQm1HFHkD8YP"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Why does Unified Embedding work?\n",
        "\n",
        "There are two reasons why Unified Embedding is a good idea.\n",
        "\n",
        "1. **Tuning table sizes:** To design individual tables for each feature, we have hundreds of knobs to tune - one per feature. In this situation, it's easy to over / under-allocate parameters, leading to lost performance. With Unified Embedding, we only have to tune the size of one table. Unified Embedding is actually *optimal* in the sense that it has about the same number of collisions as an optimally-tuned set of individual tables. This auto-tuning behavior makes hyperparameter configuration much easier.\n",
        "\n",
        "\n",
        "2. **Feature Multiplexing:** Surprisingly, we find that values from different features can share the same embedding representation without sacrificing quality. The intuition is that these features are processed by different downstream network parameters, so the model can learn to \"undo\" the collision and interpret the same embedding in different ways. We call this phenomenon *Feature Multiplexing,* and you can read more about it in our [NeurIPS 2023 paper](https://arxiv.org/abs/2305.12102).\n",
        "\n",
        "**Intuition:** Going back to our clothing retailer example, advantage #1 means that Unified Embedding protects against poorly-tuned configurations. For example, if we had a catalog of 10M products as well as a list of 500 cities, Unified Embedding will automatically allocate more buckets for the products. Advantage #2 means that if a product happens to collide with a city, the model can still correctly interpret the semantic meaning of the embedding for both features.\n"
      ]
    },
    {
      "metadata": {
        "id": "qjnpqjHCiuF8"
      },
      "cell_type": "markdown",
      "source": [
        "# Movielens 1M Example\n",
        "\n",
        "Next, let's take a look at how unified embeddings work in practice. We'll work with the Movielens-1M dataset, which is a popular benchmark in recommendation systems research. The task is to predict user movie ratings from features related to the user and the movie.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, we'll install and import the necessary packages for the code."
      ]
    },
    {
      "metadata": {
        "id": "5tHukWQsIUN5"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -1 --upgrade tensorflow-datasets"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "eH0MrPjIi6YS"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "from tensorflow_recommenders.layers.feature_multiplexing import unified_embedding\n",
        "\n",
        "UnifiedEmbedding = unified_embedding.UnifiedEmbedding\n",
        "UnifiedEmbeddingConfig = unified_embedding.UnifiedEmbeddingConfig\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3vv0xfq2F3Vw"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset\n",
        "\n",
        "Next, we'll load the movielens dataset using the Tensorflow Datasets library. This follows a similar process to the [DCN](https://www.tensorflow.org/recommenders/examples/dcn) and [basic ranking](https://www.tensorflow.org/recommenders/examples/basic_ranking) tutorials. However, instead of casting this as a regression problem (where we predict the user's movie rating), we will instead pose this as a classification (where we predict whether the user's movie rating is at least 3 stars).\n",
        "\n",
        "This is the same setup used in [our paper](https://arxiv.org/abs/2305.12102) and in the [DCNv2 paper](https://arxiv.org/abs/2008.13535).\n"
      ]
    },
    {
      "metadata": {
        "id": "8bsUbqzqF30h"
      },
      "cell_type": "code",
      "source": [
        "ratings = tfds.load(\"movie_lens/100k-ratings\", split=\"train\")\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_id\": x[\"movie_id\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_rating\": int(x[\"user_rating\"] \u003e= 3),\n",
        "    \"user_gender\": tf.strings.as_string(x[\"user_gender\"]),\n",
        "    \"user_zip_code\": x[\"user_zip_code\"],\n",
        "    \"user_occupation_text\": x[\"user_occupation_text\"],\n",
        "    \"bucketized_user_age\": tf.strings.as_string(x[\"bucketized_user_age\"]),\n",
        "})\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "dMWwELcfF7JX"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's partition the data into an 80-20 train-test split."
      ]
    },
    {
      "metadata": {
        "id": "7ceoeTTuF7QI"
      },
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Sy4Nau8jF9zC"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's compute some basic statistics (such as vocabulary size) from the dataset. We will also store the vocabulary for later, since we'll need it to build the collisionless embedding model."
      ]
    },
    {
      "metadata": {
        "id": "1c2K8cijF9Hd"
      },
      "cell_type": "code",
      "source": [
        "feature_names = [\"movie_id\", \"user_id\", \"user_gender\", \"user_zip_code\",\n",
        "                 \"user_occupation_text\", \"bucketized_user_age\"]\n",
        "\n",
        "vocabularies = {}\n",
        "for feature_name in feature_names:\n",
        "  vocab = ratings.batch(1_000_000).map(lambda x: x[feature_name])\n",
        "  vocabularies[feature_name] = np.unique(np.concatenate(list(vocab)))\n",
        "\n",
        "for feature_name in feature_names:\n",
        "  print(f\"Feature '{feature_name}' has cardinality \"\n",
        "        f\"{len(vocabularies[feature_name])}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jLwnklpSQQXX"
      },
      "cell_type": "markdown",
      "source": [
        "In a moment, we'll construct three models that differ only in their embedding component, so let's also define some convenience functions to easily construct the network and task parts of the model."
      ]
    },
    {
      "metadata": {
        "id": "gF0XJuvwQhZi"
      },
      "cell_type": "code",
      "source": [
        "def build_network(layer_sizes):\n",
        "    network_layers = []\n",
        "    # Concatenate the list of embedding features.\n",
        "    network_layers.append(tf.keras.layers.Concatenate(axis=-1))\n",
        "    # Pass the concatenated embeddings through the model.\n",
        "    for layer_size in layer_sizes:\n",
        "      network_layers.append(\n",
        "          tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "    # Add a logit layer to the top of the network.\n",
        "    network_layers.append(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "    return tf.keras.Sequential(network_layers)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bNS5rlqv1cxa"
      },
      "cell_type": "markdown",
      "source": [
        "Since we binarized the movie ratings, our recommendation problem is a binary prediction task. We'll report both the AUC and the LogLoss as metrics."
      ]
    },
    {
      "metadata": {
        "id": "zX-3IzpC1c9D"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# We will use the same task for all models.\n",
        "recsys_task = tfrs.tasks.Ranking(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(\n",
        "        reduction=tf.keras.losses.Reduction.SUM\n",
        "    ),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.AUC(name=\"AUC\"),\n",
        "        tf.keras.metrics.BinaryCrossentropy(name=\"LogLoss\"),\n",
        "    ]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jZuaM0beJvq1"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing the Models\n",
        "\n",
        "Now let's implement Keras model classes for each of the modeling strategies we talked about - collisionless embeddings, hash embeddings, and unified embeddings.\n",
        "\n",
        "### Collisionless Embedding Model\n",
        "\n",
        "For our first model, we'll explicitly assign a single embedding to each vocabulary item. This is the ideal setup where we don't have any hash collisions or representation sharing. Unfortunately, collisionless embeddings aren't feasible in large-scale production systems. We're including it as an optimistic headroom point, to give a reference point for how large the improvement could be if we were able to use a full embedding table.\n"
      ]
    },
    {
      "metadata": {
        "id": "UVcS-7vdK-rN"
      },
      "cell_type": "code",
      "source": [
        "class CollisionlessEmbeddingModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, layer_sizes, embed_dimension, vocabularies):\n",
        "    super().__init__()\n",
        "    self._embedding_dimension = embed_dimension\n",
        "    self._embedding_tables = {}\n",
        "    self._feature_names = vocabularies.keys()\n",
        "\n",
        "    for feature_name, vocab in vocabularies.items():\n",
        "      lookup_layer = tf.keras.layers.StringLookup(\n",
        "          vocabulary=vocab, mask_token=None)\n",
        "      # Collisionless embedding, without hashing.\n",
        "      embed_layer = tf.keras.layers.Embedding(\n",
        "          len(vocab) + 1, self._embedding_dimension)\n",
        "      self._embedding_tables[feature_name] = tf.keras.Sequential(\n",
        "          [lookup_layer, embed_layer])\n",
        "\n",
        "    self._network = build_network(layer_sizes)\n",
        "\n",
        "    self.task = recsys_task\n",
        "\n",
        "  def call(self, features):\n",
        "    embeddings = []\n",
        "    for feature_name in self._feature_names:\n",
        "      embedding_fn = self._embedding_tables[feature_name]\n",
        "      embeddings.append(embedding_fn(features[feature_name]))\n",
        "    return self._network(embeddings)\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    labels = features.pop(\"user_rating\")\n",
        "    scores = self(features)\n",
        "    return self.task(\n",
        "        labels=labels,\n",
        "        predictions=scores,\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "aH4BkDvaUF7s"
      },
      "cell_type": "markdown",
      "source": [
        "### Hash Embedding Model\n",
        "\n",
        "Next, we construct a model with a much smaller embedding table. This is a more realistic setup for production settings, because we cannot afford to store the full table with millions or billions of inputs. Instead, we can adjust the model size to fit our memory budget. Unfortunately, hash embeddings introduce some unavoidable representation sharing because there are more items than embeddings."
      ]
    },
    {
      "metadata": {
        "id": "Cykd0Z-VOZSd"
      },
      "cell_type": "code",
      "source": [
        "class HashEmbeddingModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, layer_sizes, embed_dimension, embed_buckets):\n",
        "    super().__init__()\n",
        "    self._embed_dimension = embed_dimension\n",
        "    self._embedding_tables = {}\n",
        "    self._feature_names = embed_buckets.keys()\n",
        "\n",
        "    for feature_name, buckets in embed_buckets.items():\n",
        "      lookup_layer = tf.keras.layers.Hashing(num_bins=buckets)\n",
        "      embed_layer = tf.keras.layers.Embedding(\n",
        "          buckets, self._embed_dimension)\n",
        "      self._embedding_tables[feature_name] = tf.keras.Sequential(\n",
        "          [lookup_layer, embed_layer])\n",
        "\n",
        "    self._network = build_network(layer_sizes)\n",
        "\n",
        "    self.task = recsys_task\n",
        "\n",
        "  def call(self, features):\n",
        "    embeddings = []\n",
        "    for feature_name in self._feature_names:\n",
        "      embedding_fn = self._embedding_tables[feature_name]\n",
        "      embeddings.append(embedding_fn(features[feature_name]))\n",
        "    return self._network(embeddings)\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    labels = features.pop(\"user_rating\")\n",
        "    scores = self(features)\n",
        "    return self.task(\n",
        "        labels=labels,\n",
        "        predictions=scores,\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "OP89FtXwQz4j"
      },
      "cell_type": "markdown",
      "source": [
        "### Unified Embedding Model\n",
        "\n",
        "Finally, let's define a class for a unified embedding model using the implementation in TFRS. The implementation is similar to the previous models, though it requires us to overload the compile method (as the embedding layer needs to know the optimizer if it is running on TPU)."
      ]
    },
    {
      "metadata": {
        "id": "rylFnZNFQ3D7"
      },
      "cell_type": "code",
      "source": [
        "translate_keras_optimizer = tfrs.layers.embedding.tpu_embedding_layer.translate_keras_optimizer\n",
        "\n",
        "class UnifiedEmbeddingModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, layer_sizes, embed_config):\n",
        "    super().__init__()\n",
        "    self._embed_config = embed_config\n",
        "    self._network = build_network(layer_sizes)\n",
        "    self.task = recsys_task\n",
        "\n",
        "  def compile(self, **kwargs):\n",
        "    # Because the embedding layer might have to run on TPU, we can only\n",
        "    # construct the layer once the optimizer is known (at model.compile() time).\n",
        "    tpu_embed_optimizer = translate_keras_optimizer(kwargs[\"optimizer\"])\n",
        "    self._embedding_layer = UnifiedEmbedding(\n",
        "        self._embed_config, tpu_embed_optimizer)\n",
        "    super().compile(**kwargs)\n",
        "\n",
        "  def call(self, features):\n",
        "    embeddings = self._embedding_layer(features)\n",
        "    return self._network(embeddings)\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    labels = features.pop(\"user_rating\")\n",
        "    scores = self(features)\n",
        "    return self.task(\n",
        "        labels=labels,\n",
        "        predictions=scores,\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "GwWHM3erXzM6"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the models\n",
        "\n",
        "Now we're ready to construct and train the models. We'll start by shuffling the training and test data.\n"
      ]
    },
    {
      "metadata": {
        "id": "YJJ7_pe_X6rc"
      },
      "cell_type": "code",
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "zctxqLw2X7HZ"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we set some hyperparameters for the network. These parameters are just for demonstration purposes and are not tuned for performance. For better results, we could tune these parameters in tandem with the choice of embedding method.\n",
        "\n",
        "We're using the legacy Keras optimizers because we need to maintain compatibility with the TPU embedding layer (in case we choose to run unified embedding on TPU). We're also going to run each model 3 times and report the mean and standard deviation for the performance metrics."
      ]
    },
    {
      "metadata": {
        "id": "jw_Kl68AX7SK"
      },
      "cell_type": "code",
      "source": [
        "layer_sizes = [128, 64]\n",
        "embed_dimension = 16\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.01\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate)\n",
        "# Number of times to independently repeat the training process.\n",
        "num_runs = 5\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "CEe5s9_g0iWC"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's create the models. We'll start with the collisionless model, which is very easy to configure."
      ]
    },
    {
      "metadata": {
        "id": "lHt7VoFxnWSU"
      },
      "cell_type": "code",
      "source": [
        "# Configure and construct the collisionless model.\n",
        "collisionless_models = []\n",
        "for _ in range(num_runs):\n",
        "  collisionless_model = CollisionlessEmbeddingModel(\n",
        "      layer_sizes, embed_dimension, vocabularies)\n",
        "  collisionless_model.compile(optimizer=optimizer)\n",
        "  collisionless_models.append(collisionless_model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "tK5FU8l11OAA"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll build the hash embedding model. This is slightly more complicated to configure, since we now have to set table sizes for each of the features. For the best practical results, we would need to tune each of these hyperparameters. Another optimization is to use collisionless tables for the low-cardinality embeddings (such as user gender and occupation). We did this for the experiments in our paper, but it takes a little more work to set up. For demonstration purposes, we've just increased the number of embeddings to be larger than the cardinality for these features."
      ]
    },
    {
      "metadata": {
        "id": "A4wWiLN81KZd"
      },
      "cell_type": "code",
      "source": [
        "# Configure and construct the hash model.\n",
        "hash_buckets = {\n",
        "    \"movie_id\": 600,\n",
        "    \"user_id\": 400,\n",
        "    \"user_gender\": 20,\n",
        "    \"user_zip_code\": 400,\n",
        "    \"user_occupation_text\": 60,\n",
        "    \"user_occupation_text\": 20,\n",
        "}\n",
        "hash_models = []\n",
        "for _ in range(num_runs):\n",
        "  hash_model = HashEmbeddingModel(layer_sizes, embed_dimension, hash_buckets)\n",
        "  hash_model.compile(optimizer=optimizer)\n",
        "  hash_models.append(hash_model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2BBUeoLM3WaH"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's build the unified embedding model. We consider a setup where we have two tables, each with half the embedding dimension, and get our embeddings by querying both tables and concatenating the results. To have a valid comparison with the hash embedding model, we construct the table so that it has the same overall number of parameters.\n",
        "\n",
        "There are other ways to configure the unified table, with some tradeoffs. For example, we could use a single lookup in a full-dimension table for each feature - this would slightly reduce the representation capacity but reduce the number of hash lookups and improve latency. In the end, the best configuration will be the one that meets the latency, memory, and performance demands of the application."
      ]
    },
    {
      "metadata": {
        "id": "8ch4ywID1NQS"
      },
      "cell_type": "code",
      "source": [
        "# Configure and construct a unified model with the same size as the hash model.\n",
        "total_buckets = sum(hash_buckets.values())\n",
        "num_tables = 2\n",
        "embed_config = UnifiedEmbeddingConfig(\n",
        "    buckets_per_table=total_buckets,\n",
        "    dim_per_table=embed_dimension // 2,\n",
        "    num_tables=num_tables,\n",
        "    name=\"unified_table\",\n",
        ")\n",
        "for feature_name in feature_names:\n",
        "  embed_config.add_feature(feature_name, 2)\n",
        "unified_models = []\n",
        "for _ in range(num_runs):\n",
        "  unified_model = UnifiedEmbeddingModel(layer_sizes, embed_config)\n",
        "  unified_model.compile(optimizer=optimizer)\n",
        "  unified_models.append(unified_model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "pIaiKqhedjSi"
      },
      "cell_type": "markdown",
      "source": [
        "For convenience, let's define a function to train several models and aggregate the outputs."
      ]
    },
    {
      "metadata": {
        "id": "_yl7x7xedinF"
      },
      "cell_type": "code",
      "source": [
        "def run_models(models):\n",
        "  aucs = []\n",
        "  losses = []\n",
        "  for model in models:\n",
        "    model.fit(cached_train, epochs=epochs, verbose=False)\n",
        "    metrics = model.evaluate(cached_test, return_dict=True, verbose=False)\n",
        "    aucs.append(metrics[\"AUC\"])\n",
        "    losses.append(metrics[\"LogLoss\"])\n",
        "  return aucs, losses"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "exAaq-uo7G4b"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "Finally, let's train the models and report the results."
      ]
    },
    {
      "metadata": {
        "id": "Mzsywy1geRWV"
      },
      "cell_type": "code",
      "source": [
        "collisionless_aucs, collisionless_losses = run_models(collisionless_models)\n",
        "hash_aucs, hash_losses = run_models(hash_models)\n",
        "unified_aucs, unified_losses = run_models(unified_models)\n",
        "\n",
        "def print_metrics(model_name, aucs, losses):\n",
        "  s = f\"{model_name} model: {np.mean(aucs):.3f} AUC (std = {np.std(aucs):.3f}) \"\n",
        "  s += f\"and {np.mean(losses):.3f} LogLoss (std = {np.std(losses):.3f}).\"\n",
        "  print(s)\n",
        "\n",
        "print_metrics(\"Collisionless\", collisionless_aucs, collisionless_losses)\n",
        "print_metrics(\"Hash Embedding\", hash_aucs, hash_losses)\n",
        "print_metrics(\"Unified Embedding\", unified_aucs, unified_losses)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4ylaWM1pwXbM"
      },
      "cell_type": "markdown",
      "source": [
        "We ended up with the following results, but there will be differences each time this is run due to the optimizer and initialization.\n",
        "\n",
        "```\n",
        "Collisionless model: 0.797 AUC (std = 0.000) and 0.380 LogLoss (std = 0.005).\n",
        "Hash Embedding model: 0.743 AUC (std = 0.001) and 0.407 LogLoss (std = 0.003).\n",
        "Unified Embedding model: 0.790 AUC (std = 0.001) and 0.380 LogLoss (std = 0.001).\n",
        "```\n",
        "\n",
        "Unified embedding should have about 4-5% higher AUC when compared to the hash embeddings, while having the same overall parameter budget. The performance is close to the collisionless model, but only uses about 57% of the parameters (43k vs 76k).\n",
        "\n",
        "To learn more about how the method works, you may want to play around with the unified embedding configuration. For example, using only one table (no chunking) will reduce the AUC by about 2%. It should also be possible to increase the performance of the hash embedding model by carefully tuning the table sizes. It's also interesting to see how small the models can go - for very small table sizes, unified embedding should have even larger gains.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "We hope you developed some intuition about feature multiplexing and enjoyed learning how unified embedding can work in practice. To learn more, you can check out our NeurIPS paper, [\"Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems.\"](https://arxiv.org/abs/2305.12102)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
