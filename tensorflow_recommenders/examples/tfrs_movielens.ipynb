{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCeYA79m1DEX"
   },
   "source": [
    "# Building a movie retrieval system\n",
    "\n",
    "In this tutorial, we're going to build an end-to-end retrieval system for movies. \n",
    "\n",
    "A retrieval system is normally the first stage in a multi-stage recommender system and is responsible for retrieving a set of candidates out of a large corpus in response to a user query.\n",
    "\n",
    "Retrieval models are often composed of two sub-models:\n",
    "\n",
    "1. A query model computing the query representation (normally a fixed-dimensionality embedding vector) using query features.\n",
    "2. A candidate model computing the candidate representation (an equally-sized vector) using the candidate features\n",
    "\n",
    "The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query.\n",
    "\n",
    "In this tutorial, we're going to build and train such a two-tower model using the Movielens dataset.\n",
    "\n",
    "We're going to:\n",
    "\n",
    "1. Get our data and split it into a training and test set.\n",
    "2. Implement a retrieval model.\n",
    "3. Fit and evaluate it.\n",
    "4. Export it for efficient serving by building an approximate nearest neighbours (ANN) index.\n",
    "\n",
    "## The dataset\n",
    "\n",
    "The Movielens dataset is a classic dataset from the [GroupLens](https://grouplens.org/datasets/movielens/) research group at the University of Minnesota. It contains a set of ratings given to movies by a set of users, and is a workhorse of recommender system research.\n",
    "\n",
    "The data can be treated in two ways:\n",
    "\n",
    "1. It can be interpreted as expressesing which movies the users watched (and rated), and which they did not. This is a form of implicit feedback, where users' watches tell us which things they prefer to see and which they'd rather not see.\n",
    "2. It can also be seen as expressesing how much the users liked the movies they did watch. This is a form of explicit feedback: given that a user watched a movie, we can tell roughly how much they liked by looking at the rating they have given.\n",
    "\n",
    "In this tutorial, we are focusing on a retrieval system: a model that predicts a set of movies from the catalogue that the user is likely to watch. Often, implicit data is more useful here, and so we are going to treat Movielens as an implicit system. This means that every movie a user watched is a positive example, and every movie they have not seen is an implicit negative example.\n",
    "\n",
    "\n",
    "## Imports\n",
    "\n",
    "\n",
    "Let's first get our imports out of the way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZGYDaF-m5wZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxQ_hy7xPH3N"
   },
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PAqjR4a1RR4"
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "Let's first have a look at the data.\n",
    "\n",
    "We use the MovieLens dataset from [Tensorflow Datasets](https://www.tensorflow.org/datasets). Specifically, loading `movie_lens/100k_ratings` yields a `tf.data.Dataset` object containing the ratings data and loading `movie_lens/100k_movies` yields a `tf.data.Dataset` object containing only the movies data.\n",
    "\n",
    "Note that since the MovieLens dataset does not have predefined splits, all data are under `train` split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaQhqcLGP0jL"
   },
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movie_lens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movie_lens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRHorm8W1yf3"
   },
   "source": [
    "The ratings dataset returns a dictionary of movie id, user id, the assigned rating, timestamp, movie information, and user information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "_1-KQV2ynMdh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n",
      "{'bucketized_user_age': 25.0,\n",
      " 'movie_genres': array([ 4, 14]),\n",
      " 'movie_id': b'709',\n",
      " 'movie_title': b'Strictly Ballroom (1992)',\n",
      " 'raw_user_age': 32.0,\n",
      " 'timestamp': 875654590,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'92',\n",
      " 'user_occupation_label': 5,\n",
      " 'user_occupation_text': b'entertainment',\n",
      " 'user_rating': 2.0,\n",
      " 'user_zip_code': b'80525'}\n",
      "{'bucketized_user_age': 18.0,\n",
      " 'movie_genres': array([4]),\n",
      " 'movie_id': b'412',\n",
      " 'movie_title': b'Very Brady Sequel, A (1996)',\n",
      " 'raw_user_age': 24.0,\n",
      " 'timestamp': 882075110,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'301',\n",
      " 'user_occupation_label': 17,\n",
      " 'user_occupation_text': b'student',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'55439'}\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qGLGCjSt_q96"
   },
   "source": [
    "The movies dataset contains the movie id, movie title, and data on what genres it belongs to. Note that the genres are encoded with integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "kHLsIHhw_x1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUdT-f4RxMKs"
   },
   "source": [
    "In this example, we're going to focus on the ratings data. Other tutorials explore how to use the movie information data as well to improve the model quality.\n",
    "\n",
    "We keep only the `movie_id`, `user_id`, and `movie_title` fields in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhbEvPJqxLec"
   },
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "})\n",
    "movies = movies.map(lambda x: {\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iu4XSa_G1nyN"
   },
   "source": [
    "To fit and evaluate the model, we need to split it into a training and evaluation set. In an industrial recommender system, this would most likely be done by time: the data up to time $T$ would be used to predict interactions after $T$.\n",
    "\n",
    "\n",
    "In this simple example, however, let's use a random split, putting 80% of the ratings in the train set, and 20% in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rS0eDfkjnjJL"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVi1HJfR9D7H"
   },
   "source": [
    "Let's also figure out unique user ids and movie ids present in the data. \n",
    "\n",
    "This is important because we need to be able to map the raw values of our categorical features to embedding vectors in our models. To do that, we need a vocabulary that maps a raw feature value to an integer in a contiguous range: this allows us to look up the corresponding embeddings in our embedding tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MKROCiPo_5LJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '10', '100', '1000', '1001', '1002', '1003', '1004', '1005', '1006']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_ids = movies.batch(1_000).map(lambda x: x[\"movie_id\"])\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_ids = np.unique(np.concatenate(list(movie_ids)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "# We convert bytes to strings since bytes are not serializable\n",
    "unique_movie_id_strings = [id.decode(\"utf-8\") for id in unique_movie_ids]\n",
    "unique_user_id_strings = [id.decode(\"utf-8\") for id in unique_user_ids]\n",
    "\n",
    "unique_movie_id_strings[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCi-seR86qqa"
   },
   "source": [
    "## Implementing a model\n",
    "\n",
    "This is the critical section where we choose the architecure of our model.\n",
    "\n",
    "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z20PyfSXP3Um"
   },
   "source": [
    "### The query tower\n",
    "\n",
    "Let's start with the query tower.\n",
    "\n",
    "The first step is to decide on the dimensionality of the query and candidate representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbIy1FP8aCTq"
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJYwjpLRaEzj"
   },
   "source": [
    "The second is to define the input features. Here, we're going to use [feature columns](https://www.tensorflow.org/tutorials/structured_data/feature_columns), to define a simple embedding layer, taking `user_id` as its only input feature. Note that we use the list of unique user ids we computed earlier as a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHQZJEhXP93N"
   },
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, embedding_dimension):\n",
    "    super(UserModel, self).__init__()\n",
    "    # The model itself is a single embedding layer.\n",
    "    # However, we could expand this to an arbitrarily complicated Keras model, as long\n",
    "    # as the output is an vector `embedding_dimension` wide.\n",
    "    user_features = [tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            \"user_id\", unique_user_id_strings,\n",
    "        ),\n",
    "        embedding_dimension,\n",
    "    )]\n",
    "    self.embedding_layer = tf.keras.layers.DenseFeatures(user_features, name=\"user_embedding\")\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return self.embedding_layer(inputs)\n",
    "\n",
    "# We initialize these models and later pass them to the full model.\n",
    "user_model = UserModel(embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qvo2pEcdaiec"
   },
   "source": [
    "A simple model like this corresponds exactly to a classic [matrix factorization](https://ieeexplore.ieee.org/abstract/document/4781121) approach. Defining a subclass of `tf.keras.Model` for this simple model might be an overkill. However, we could easily extend it to an arbitrarily complex model using standard Keras components, as long as we return an `embedding_dimension`-wide output at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dG4YFy9SQ08d"
   },
   "source": [
    "### The candidate tower\n",
    "\n",
    "We can do the same with the candidate tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qNUwfIJTQ332"
   },
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, embedding_dimension):\n",
    "    super(MovieModel, self).__init__()\n",
    "    movie_features = [tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            \"movie_id\", unique_movie_id_strings,\n",
    "        ),\n",
    "        embedding_dimension,\n",
    "    )]\n",
    "    self.embedding_layer = tf.keras.layers.DenseFeatures(movie_features, name=\"movie_embedding\")\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    return self.embedding_layer(inputs)\n",
    "\n",
    "movie_model = MovieModel(embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r10RiPtqVIAl"
   },
   "source": [
    "### Metrics\n",
    "\n",
    "In our training data we have positive (user, movie) pairs. To figure out how good our model is, we need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
    "\n",
    "To do this, we can use the `tfrs.metrics.FactorizedTopK` metric. The metric has one required argument: the dataset of candidates that are used as implicit negatives for evaluation.\n",
    "\n",
    "In our case, that's the `movies` dataset, converted into embeddings via our movie model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1dLDL6pZVPO8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=movies.batch(128).map(lambda x: {\"movie_id\": x[\"movie_id\"]}).map(movie_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCaCqJsXSkCo"
   },
   "source": [
    "### Loss\n",
    "\n",
    "The next component is the loss used to train our model. TFRS has several loss layers and tasks to make this easy.\n",
    "\n",
    "In this instance, we'll make use of the `RetrievalTask` object: a convenience wrapper that bundles together the loss function and metric computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJ61Iz2QTBw3"
   },
   "outputs": [],
   "source": [
    "task = tfrs.tasks.RetrievalTask(\n",
    "  corpus_metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-3xFC-1cbz0"
   },
   "source": [
    "The task itself is a Keras layer that takes the query and candidate embeddings as arguments, and returns the computed loss: we'll use that to implement the model's training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZUFeSlWRHGx"
   },
   "source": [
    "### The full model\n",
    "\n",
    "We can now put it all together into a model. TFRS exposes a base model class `tfrs.models.Model` which streamlines bulding models: all we need to do is to set up the components in the `__init__` method, and implement the `compute_loss` method, taking in the raw features and returning a loss value.\n",
    "\n",
    "The base model will then take care of creating the appropriate training loop to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8n7c5CHFp0ow"
   },
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model({\"user_id\": features[\"user_id\"]})\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_movie_embeddings = self.movie_model({\"movie_id\": features[\"movie_id\"]})\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDN_LJGlnRGo"
   },
   "source": [
    "## Fitting and evaluating\n",
    "\n",
    "After defining the model, we can use standard Keras fitting and evaluation routines to fit and evaluate the model.\n",
    "\n",
    "Let's first instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aW63YaqP2wCf"
   },
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nma0vc2XdN5g"
   },
   "source": [
    "Then shuffle, batch, and cache the training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53QJwY1gUnfv"
   },
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8mHTxKAdTJO"
   },
   "source": [
    "Then train the  model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "ZxPntlT8EFOZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 20s 2s/step - factorized_top_k: 0.0233 - factorized_top_k/top_1_categorical_accuracy: 1.6250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0029 - factorized_top_k/top_10_categorical_accuracy: 0.0063 - factorized_top_k/top_50_categorical_accuracy: 0.0351 - factorized_top_k/top_100_categorical_accuracy: 0.0722 - loss: 70263.3210\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 20s 2s/step - factorized_top_k: 0.0977 - factorized_top_k/top_1_categorical_accuracy: 0.0011 - factorized_top_k/top_5_categorical_accuracy: 0.0145 - factorized_top_k/top_10_categorical_accuracy: 0.0321 - factorized_top_k/top_50_categorical_accuracy: 0.1599 - factorized_top_k/top_100_categorical_accuracy: 0.2809 - loss: 67848.3388\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 20s 2s/step - factorized_top_k: 0.1172 - factorized_top_k/top_1_categorical_accuracy: 0.0019 - factorized_top_k/top_5_categorical_accuracy: 0.0214 - factorized_top_k/top_10_categorical_accuracy: 0.0460 - factorized_top_k/top_50_categorical_accuracy: 0.1943 - factorized_top_k/top_100_categorical_accuracy: 0.3226 - loss: 66375.4304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f521b6d45f8>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsluR8audV9W"
   },
   "source": [
    "As the model trains, the loss is falling and a set of top-k retrieval metrics is updated. These tell us whether the true positive is in the top-k retrieved items from the entire candidate set. For example, a top-5 categorical accuracy metric of 0.2 would tell us that, on average, the true positive is in the top 5 retrieved items 20% of the time.\n",
    "\n",
    "Note that, in this example, we evaluate the metrics during training as well as evaluation. Because this can be quite slow with large candidate sets, it may be prudent to turn metric calculation off in training, and only run it in evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Gxp5RLFcv64"
   },
   "source": [
    "Finally, we can evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "W-zu6HLODNeI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 708ms/step - factorized_top_k: 0.0644 - factorized_top_k/top_1_categorical_accuracy: 6.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0060 - factorized_top_k/top_10_categorical_accuracy: 0.0151 - factorized_top_k/top_50_categorical_accuracy: 0.0985 - factorized_top_k/top_100_categorical_accuracy: 0.2016 - loss: 31400.3115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k': array([0.00065, 0.006  , 0.0151 , 0.0985 , 0.2016 ], dtype=float32),\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.20160000026226044,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.01510000042617321,\n",
       " 'factorized_top_k/top_1_categorical_accuracy': 0.0006500000017695129,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.09849999845027924,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.006000000052154064,\n",
       " 'loss': 28554.8203125}"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKZyP9A1dxit"
   },
   "source": [
    "Test set performance is much worse than training performance. This is due to two factors:\n",
    "\n",
    "1. Our model is likely to perform better on the data that it has seen, simply because it can memorize it. This overfitting phenomenon is especially strong when models have many parameters. It can be mediated by model regularization and use of user and movie features that help the model generalize better to unseen data.\n",
    "2. The model is re-recommending some of users' already watched movies. These known-positive watches can crowd out test movies out of top K recommendations.\n",
    "\n",
    "The second phenomenon can be tackled by excluding previously seen movies from test recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R5WBNgdAdvJk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_at_k': 0.09097664543524417, 'recall_at_k': 0.06814190082266666}"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrs.examples.movielens.evaluate(\n",
    "    user_model=model.user_model,\n",
    "    movie_model=model.movie_model,\n",
    "    test=test,\n",
    "    movies=movies,\n",
    "    train=train,\n",
    "    k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TrS6pOzveZjM"
   },
   "source": [
    "These values are higher than if we did not exclude the training set watches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AaHbFcQteeYL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_at_k': 0.03343949044585987, 'recall_at_k': 0.02842858616814186}"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrs.examples.movielens.evaluate(\n",
    "    user_model=model.user_model,\n",
    "    movie_model=model.movie_model,\n",
    "    test=test,\n",
    "    movies=movies,\n",
    "    train=None,\n",
    "    k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HP1JtIBPgOxg"
   },
   "source": [
    "Of course, accuracy on the training set is still much higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "USFkEXEXgStK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_at_k': 0.6022269353128314, 'recall_at_k': 0.12978213265599212}"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrs.examples.movielens.evaluate(\n",
    "    user_model=model.user_model,\n",
    "    movie_model=model.movie_model,\n",
    "    test=train,\n",
    "    movies=movies,\n",
    "    k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NB2v43NJU3Xf"
   },
   "source": [
    "## Making predictions\n",
    "\n",
    "Now that we have a model, we would like to be able to make predictions. We can use the `DatasetIndexedTopK` layer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRD6bEtZW_8j"
   },
   "outputs": [],
   "source": [
    "top_k = tfrs.layers.corpus.DatasetIndexedTopK(\n",
    "    # We transform the movies dataset into pairs of (movie title, movie embedding)\n",
    "    # to allow us to retrieve most highly scored titles given embedding.\n",
    "    # We use the `cache` transformation to make sure we don't recompute\n",
    "    # movie embeddings every time we score a query.\n",
    "    candidates=movies.batch(4096).map(lambda x: (\n",
    "        x[\"movie_title\"],\n",
    "        model.movie_model({\"movie_id\": x[\"movie_id\"]})\n",
    "    )).cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPYT_2isZ4C-"
   },
   "source": [
    "Now that we have the candidate layer, all that remains is to get some user embeddings and run the top k queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "fGgTesLlX7Vl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top titles for user 10: [[b'Blue Angel, The (Blaue Engel, Der) (1930)' b'Big Sleep, The (1946)'\n",
      "  b'Cat on a Hot Tin Roof (1958)' b\"Singin' in the Rain (1952)\"\n",
      "  b'Apartment, The (1960)' b'To Catch a Thief (1955)'\n",
      "  b'Treasure of the Sierra Madre, The (1948)'\n",
      "  b'Wonderful, Horrible Life of Leni Riefenstahl, The (1993)'\n",
      "  b'All About Eve (1950)' b'M (1931)']]\n",
      "Top titles for user 123: [[b'Annie Hall (1977)' b\"Antonia's Line (1995)\" b'Secrets & Lies (1996)'\n",
      "  b'Blue Angel, The (Blaue Engel, Der) (1930)' b'Roman Holiday (1953)'\n",
      "  b'Treasure of the Sierra Madre, The (1948)'\n",
      "  b'Philadelphia Story, The (1940)' b'Penny Serenade (1941)'\n",
      "  b'It Happened One Night (1934)' b'Ruby in Paradise (1993)']]\n",
      "Top titles for user 557: [[b'Welcome To Sarajevo (1997)' b'Love Jones (1997)'\n",
      "  b'House of Yes, The (1997)' b\"Eve's Bayou (1997)\"\n",
      "  b'In the Company of Men (1997)' b'Hoodlum (1997)' b'Money Talks (1997)'\n",
      "  b\"She's So Lovely (1997)\" b'Jackie Brown (1997)'\n",
      "  b'Ice Storm, The (1997)']]\n"
     ]
    }
   ],
   "source": [
    "for user_id in (\"10\", \"123\", \"557\"):\n",
    "  _, top_titles = top_k(model.user_model({\"user_id\": np.array([user_id])}))\n",
    "  print(f\"Top titles for user {user_id}: {top_titles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFlvp5DK4Ow8"
   },
   "source": [
    "## Model serving\n",
    "\n",
    "After the model is trained, we need a way to deploy it.\n",
    "\n",
    "In a two-tower retrieval model, serving has two components:\n",
    "\n",
    "- a serving query model, taking in features of the query and transforming them into a query embedding, and\n",
    "- a serving candidate model. This most often takes the form of an approximate nearest neighbours (ANN) index which allows fast approximate lookup of candidates in response to a query produced by the query model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIJ2GK9k-vHe"
   },
   "source": [
    "### Exporting a query model to serving\n",
    "\n",
    "Exporting the query model is easy: we can either serialize the Keras model directly, or export it to a `SavedModel` format to make it possible to serve using [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving).\n",
    "\n",
    "To export to a `SavedModel` format, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "wi72fBjan0JU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgwrqanq9/query_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgwrqanq9/query_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: [-0.01021871  0.4163845  -0.7192347 ]\n"
     ]
    }
   ],
   "source": [
    "# Export the query model.\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "  path = os.path.join(tmp, \"query_model\")\n",
    "  tf.saved_model.save(model.user_model, path)\n",
    "  loaded = tf.saved_model.load(path)\n",
    "  infer = loaded.signatures[\"serving_default\"]\n",
    "  query_embedding = infer(user_id=tf.constant([\"10\"], dtype=tf.string))[\"output_1\"]\n",
    "\n",
    "  print(f\"Query embedding: {query_embedding[0, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-ENamtGmLHY"
   },
   "source": [
    "### Building a candidate ANN index\n",
    "\n",
    "Exporting candidate representations is more involved. Firstly, we want to pre-compute them to make sure serving is fast; this is especially important if the candidate model is computationally intensive (for example, if it has many or wide layers; or uses complex representations for text or images). Secondly, we would like to take the precomputed representations and use them to construct a fast approximate retrieval index.\n",
    "\n",
    "\n",
    "We can use [Annoy](https://github.com/spotify/annoy) to build such an index.\n",
    "\n",
    "We first install annoy in case it is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pc0JvCaV8v4K"
   },
   "outputs": [],
   "source": [
    "!pip3 install annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8AAuhapJZ_j"
   },
   "source": [
    "We now instantiate the index object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jghIJ0ydqT6R"
   },
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "index = AnnoyIndex(embedding_dimension, \"dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YdojJq9m5kmZ"
   },
   "source": [
    "Then take the candidate dataset and transform its raw features into embeddings using the movie model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZnsvraK5v_a"
   },
   "outputs": [],
   "source": [
    "movie_embeddings = (\n",
    "    movies.batch(128).map(lambda x: (\n",
    "        tf.strings.to_number(x[\"movie_id\"], out_type=tf.int32),\n",
    "        model.movie_model({\"movie_id\": x[\"movie_id\"]}),\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AStmvt_I5_hx"
   },
   "source": [
    "And then index the movie_id, movie embedding pairs into our Annoy index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H9olnuVrmZxa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_id_to_title = dict(movies.map(\n",
    "    lambda x: (tf.strings.to_number(x[\"movie_id\"], out_type=tf.int32), x[\"movie_title\"])\n",
    ").as_numpy_iterator())\n",
    "\n",
    "# We unbatch the dataset because Annoy accepts only scalar (id, embedding) pairs.\n",
    "for movie_id, movie_embedding in movie_embeddings.unbatch().as_numpy_iterator():\n",
    "  index.add_item(movie_id, movie_embedding)\n",
    "\n",
    "# Build a 10-tree ANN index.\n",
    "index.build(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y04_XP_j7CIn"
   },
   "source": [
    "We can then retrieve nearest neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "4U3vr8aWnPTm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: [b'Hard Target (1993)', b'Rising Sun (1993)', b'Dangerous Minds (1995)'].\n",
      "Candidates: [b'Air Bud (1997)', b\"McHale's Navy (1997)\", b'Jungle2Jungle (1997)'].\n",
      "Candidates: [b'Tom and Huck (1995)', b'Heavyweights (1994)', b'Another Stakeout (1993)'].\n",
      "Candidates: [b'Affair to Remember, An (1957)', b'French Kiss (1995)', b'American President, The (1995)'].\n",
      "Candidates: [b'Raiders of the Lost Ark (1981)', b'Terminator 2: Judgment Day (1991)', b'Forbidden Planet (1956)'].\n",
      "Candidates: [b'City of Lost Children, The (1995)', b'Breaking the Waves (1996)', b'Godfather, The (1972)'].\n",
      "Candidates: [b'Garden of Finzi-Contini, The (Giardino dei Finzi-Contini, Il) (1970)', b'World of Apu, The (Apur Sansar) (1959)', b'Belle de jour (1967)'].\n",
      "Candidates: [b'Affair to Remember, An (1957)', b'French Kiss (1995)', b'American President, The (1995)'].\n",
      "Candidates: [b'Substitute, The (1996)', b'Quest, The (1996)', b'Sudden Death (1995)'].\n",
      "Candidates: [b'Mortal Kombat: Annihilation (1997)', b'Mortal Kombat (1995)', b'Assassins (1995)'].\n"
     ]
    }
   ],
   "source": [
    "for row in test.batch(1).take(10):\n",
    "  query_embedding = model.user_model(\n",
    "      {\"user_id\": row[\"user_id\"]}\n",
    "  )[0]\n",
    "  candidates = index.get_nns_by_vector(query_embedding, 3)\n",
    "  print(f\"Candidates: {[movie_id_to_title[x] for x in candidates]}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ws4w5jQ3fX87"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "This concludes the retrieval tutorial.\n",
    "\n",
    "To expand on what is presented here, have a look at:\n",
    "\n",
    "1. Learning multi-task models: jointly optimizing for ratings and clicks.\n",
    "2. Using movie metadata: building a more complex movie model to alleviate cold-start."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tfrs_movielens.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
